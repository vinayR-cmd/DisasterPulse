{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "364OM4rK56BW",
        "outputId": "3f3ab895-e596-4a10-e63e-aed2a836e3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-translate==3.15.3\n",
            "  Downloading google_cloud_translate-3.15.3-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-translate==3.15.3) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate==3.15.3) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate==3.15.3) (2.4.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate==3.15.3) (1.26.1)\n",
            "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-translate==3.15.3)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-translate==3.15.3) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-translate==3.15.3) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-translate==3.15.3) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-translate==3.15.3) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-translate==3.15.3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-translate==3.15.3) (4.9.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-translate==3.15.3)\n",
            "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-translate==3.15.3) (0.6.1)\n",
            "Downloading google_cloud_translate-3.15.3-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: protobuf, grpcio-status, google-cloud-translate\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.71.2\n",
            "    Uninstalling grpcio-status-1.71.2:\n",
            "      Successfully uninstalled grpcio-status-1.71.2\n",
            "  Attempting uninstall: google-cloud-translate\n",
            "    Found existing installation: google-cloud-translate 3.21.1\n",
            "    Uninstalling google-cloud-translate-3.21.1:\n",
            "      Successfully uninstalled google-cloud-translate-3.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-cloud-translate-3.15.3 grpcio-status-1.62.3 protobuf-4.25.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "1db1da2313864dbc989a13fd47293fc9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ===========================\n",
        "# 0. Install dependencies\n",
        "# ===========================\n",
        "!pip install google-cloud-translate==3.15.3 transformers torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final pipeline starts"
      ],
      "metadata": {
        "id": "lSkKovPur4A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVM2UD4UdAPD",
        "outputId": "1d0e6f55-8dfa-4ef3-d013-bfefba9e53e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.49.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.25.8)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from PIL import Image, ExifTags\n",
        "from PIL.ExifTags import TAGS, GPSTAGS\n",
        "from google.cloud import translate_v2 as translate\n",
        "import tweepy\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Google Translate setup\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/sihproject-471916-514170694ad5.json\"\n",
        "translate_client = translate.Client()\n",
        "\n",
        "# Tweepy setup\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAKl%2B4AEAAAAA1RIimtdT1mw73boVJogtxxVNQJU%3DgbN1hecgD6OmRFBqBxooFqAja1dy1bPlGT4d7FXyVoCLUDgzaw\"\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# Hazard keywords\n",
        "hazard_keywords = [\"flood\", \"tsunami\", \"storm\", \"earthquake\", \"cyclone\", \"landslide\", \"fire\", \"oil spill\", \"shipwreck\"]\n",
        "\n",
        "# Zero-shot classifier setup\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# ---------------- EXIF + GPS Functions ----------------\n",
        "def get_exif_data(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    exif_data = {}\n",
        "    info = image._getexif()\n",
        "    if not info:\n",
        "        return None\n",
        "    for tag, value in info.items():\n",
        "        decoded = TAGS.get(tag, tag)\n",
        "        if decoded == \"GPSInfo\":\n",
        "            gps_data = {}\n",
        "            for t in value:\n",
        "                gps_decoded = GPSTAGS.get(t, t)\n",
        "                gps_data[gps_decoded] = value[t]\n",
        "            exif_data[decoded] = gps_data\n",
        "        else:\n",
        "            exif_data[decoded] = value\n",
        "    return exif_data\n",
        "\n",
        "def to_float(rational):\n",
        "    try:\n",
        "        return float(rational)\n",
        "    except TypeError:\n",
        "        return rational.numerator / rational.denominator\n",
        "\n",
        "def convert_to_degrees(value):\n",
        "    d, m, s = value\n",
        "    return to_float(d) + to_float(m)/60 + to_float(s)/3600\n",
        "\n",
        "def get_lat_lon(exif_data):\n",
        "    if not exif_data or \"GPSInfo\" not in exif_data:\n",
        "        return None, None\n",
        "    gps_info = exif_data[\"GPSInfo\"]\n",
        "    lat = convert_to_degrees(gps_info[\"GPSLatitude\"])\n",
        "    if gps_info[\"GPSLatitudeRef\"] != \"N\":\n",
        "        lat = -lat\n",
        "    lon = convert_to_degrees(gps_info[\"GPSLongitude\"])\n",
        "    if gps_info[\"GPSLongitudeRef\"] != \"E\":\n",
        "        lon = -lon\n",
        "    return lat, lon\n",
        "\n",
        "def reverse_geocode(lat, lon):\n",
        "    url = f\"https://nominatim.openstreetmap.org/reverse?lat={lat}&lon={lon}&format=json\"\n",
        "    response = requests.get(url, headers={\"User-Agent\": \"my-app\"}).json()\n",
        "    address = response.get(\"address\", {})\n",
        "    city = address.get(\"city\") or address.get(\"town\") or address.get(\"village\") or None\n",
        "    full_address = response.get(\"display_name\", \"Unknown\")\n",
        "    return city, full_address\n",
        "\n",
        "# ---------------- Hazard & Tweets ----------------\n",
        "def classify_hazard(description, target_language=\"en\"):\n",
        "    result = translate_client.translate(description, target_language=target_language)\n",
        "    translated_text = result[\"translatedText\"].lower()\n",
        "    scores = {}\n",
        "    for hazard in hazard_keywords:\n",
        "        scores[hazard] = 1.0 if hazard in translated_text else 0.0\n",
        "    best_hazard = max(scores, key=scores.get)\n",
        "    return best_hazard, scores\n",
        "\n",
        "def detect_city_from_text(text):\n",
        "    result = translate_client.translate(text, target_language=\"en\")\n",
        "    translated_text = result[\"translatedText\"].lower()\n",
        "    words = translated_text.split()\n",
        "    city = words[0]  # fallback, can improve with full geocoding\n",
        "    return city\n",
        "\n",
        "def fetch_tweets(city, hazard, max_results=10):\n",
        "    query = f'(\"{city}\" OR #{city}) (\"{hazard}\" OR #{hazard}) -is:retweet'\n",
        "    print(\"Twitter Query:\", query)\n",
        "\n",
        "    now = datetime.utcnow()\n",
        "    start_time = (now - timedelta(hours=1)).isoformat(\"T\") + \"Z\"\n",
        "    end_time   = (now - timedelta(seconds=15)).isoformat(\"T\") + \"Z\"\n",
        "\n",
        "    local_tz = pytz.timezone(\"Asia/Kolkata\")\n",
        "\n",
        "    tweets = client.search_recent_tweets(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        tweet_fields=[\"created_at\", \"text\", \"geo\"],\n",
        "        expansions=[\"author_id\"],\n",
        "        user_fields=[\"username\"],\n",
        "        start_time=start_time,\n",
        "        end_time=end_time\n",
        "    )\n",
        "\n",
        "    user_map = {}\n",
        "    if tweets.includes and \"users\" in tweets.includes:\n",
        "        for u in tweets.includes[\"users\"]:\n",
        "            user_map[u[\"id\"]] = u[\"username\"]\n",
        "\n",
        "    tweets_texts = []\n",
        "    if tweets.data:\n",
        "        for t in tweets.data:\n",
        "            local_time = t.created_at.astimezone(local_tz)\n",
        "            formatted_time = local_time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "            username = user_map.get(t.author_id, \"Unknown\")\n",
        "            print(f\"[{formatted_time}] @{username} → {t.text}\\n\")\n",
        "            tweets_texts.append(t.text)\n",
        "    else:\n",
        "        print(\"No tweets found for this location + hazard in the last 1 hour.\")\n",
        "\n",
        "    return tweets_texts\n",
        "\n",
        "# ---------------- Classification ----------------\n",
        "def translate_tweets_to_english(tweets_list):\n",
        "    translated_tweets = []\n",
        "    for tweet in tweets_list:\n",
        "        result = translate_client.translate(tweet, target_language=\"en\")\n",
        "        translated_text = result[\"translatedText\"]\n",
        "        translated_tweets.append(translated_text)\n",
        "    return translated_tweets\n",
        "\n",
        "def classify_tweets_disaster(translated_tweets):\n",
        "    aggregate_scores = {hazard: 0.0 for hazard in hazard_keywords}\n",
        "    for tweet in translated_tweets:\n",
        "        res = classifier(tweet, hazard_keywords, multi_label=False)\n",
        "        top_label = res['labels'][0]\n",
        "        top_score = res['scores'][0]\n",
        "        aggregate_scores[top_label] += top_score\n",
        "    num_tweets = len(translated_tweets)\n",
        "    if num_tweets == 0:\n",
        "        return None, None, None\n",
        "    for hazard in aggregate_scores:\n",
        "        aggregate_scores[hazard] = (aggregate_scores[hazard] / num_tweets) * 100\n",
        "    final_disaster = max(aggregate_scores, key=aggregate_scores.get)\n",
        "    intensity = aggregate_scores[final_disaster]\n",
        "    return final_disaster, intensity, aggregate_scores\n",
        "\n",
        "# ---------------- Pipeline ----------------\n",
        "def analyze_disaster_from_tweets(tweets_texts):\n",
        "    if not tweets_texts:\n",
        "        print(\"No tweets to analyze.\")\n",
        "        return None, None, None\n",
        "    print(\"Translating tweets to English...\")\n",
        "    translated_tweets = translate_tweets_to_english(tweets_texts)\n",
        "    print(\"Classifying tweets for disaster type...\")\n",
        "    final_disaster, intensity, scores = classify_tweets_disaster(translated_tweets)\n",
        "    print(\"\\n=== Disaster Analysis from Tweets ===\")\n",
        "    print(\"Final Disaster Type:\", final_disaster)\n",
        "    print(f\"Disaster Intensity (Confidence): {intensity:.2f}%\")\n",
        "    print(\"Detailed Scores per Hazard:\", scores)\n",
        "    return final_disaster, intensity, scores\n",
        "\n",
        "def run_pipeline(image_path=None, description=None, direct_text=None):\n",
        "    if direct_text:\n",
        "        print(\"\\nRunning Direct Text Search Mode...\")\n",
        "        hazard, _ = classify_hazard(direct_text)\n",
        "        city = detect_city_from_text(direct_text)\n",
        "        print(\"Detected City:\", city)\n",
        "        print(\"Predicted Hazard:\", hazard)\n",
        "        tweets_texts = fetch_tweets(city, hazard)\n",
        "        return analyze_disaster_from_tweets(tweets_texts)\n",
        "\n",
        "    elif image_path and description:\n",
        "        print(\"\\nRunning Image + Description Mode...\")\n",
        "        exif_data = get_exif_data(image_path)\n",
        "        lat, lon = get_lat_lon(exif_data)\n",
        "        if lat and lon:\n",
        "            city, full_address = reverse_geocode(lat, lon)\n",
        "            print(\"Detected City:\", city)\n",
        "            print(\"Full Address:\", full_address)\n",
        "        else:\n",
        "            city = detect_city_from_text(description)\n",
        "            print(\"Detected City from text:\", city)\n",
        "        hazard, _ = classify_hazard(description)\n",
        "        print(\"Predicted Hazard:\", hazard)\n",
        "        tweets_texts = fetch_tweets(city, hazard)\n",
        "        return analyze_disaster_from_tweets(tweets_texts)\n",
        "\n",
        "    else:\n",
        "        print(\"Provide either image+description or direct_text input.\")\n",
        "        return None, None, None\n",
        "\n",
        "# ---------------- Example Run ----------------\n",
        "run_pipeline(direct_text=\"पंजाब बाढ़\")\n",
        "# run_pipeline(image_path=\"/content/drive/MyDrive/IMG20250904195552.jpg\", description=\"बाढ़ आ गई है और पानी बहुत बढ़ रहा है\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyU-8FoSJprO",
        "outputId": "245d624f-2c87-4c13-96fd-395628a12f55"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Direct Text Search Mode...\n",
            "Detected City: punjab\n",
            "Predicted Hazard: flood\n",
            "Twitter Query: (\"punjab\" OR #punjab) (\"flood\" OR #flood) -is:retweet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2244997275.py:100: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.utcnow()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-20 01:09:28 IST] @KaptaanWelfare → South Punjab ke flood se affected logon ki madad karna humari zimmedari hai. Din raat khana, pani aur zaroori cheezein pohchane ki koshish kar rhy hain. Aap ki choti si support kisi ki zindagi bacha sakti hai. ❤️🙏 \n",
            "#kaptaanwelfaresociety\n",
            "#floodrelief #southpunjab #welfarework https://t.co/OCrPwX7wYE\n",
            "\n",
            "[2025-09-20 00:27:10 IST] @NaurangD86736 → बाढ़ पीड़ित गाँव में लगातार मदद की जा रही है | अन्नपूर्णा मुहिम | #flood #punjab https://t.co/WhhDP4nmRb\n",
            "\n",
            "[2025-09-20 00:20:21 IST] @Arvind881161 → @Her_Harpreet @Toxicity_______ @narendramodi @HarshdeepKaur Congress and aap never brought msp, never solved a single problem for Punjab, aap govt did not help during flood...but still you guys find excuses to hate Modi and BJP.\n",
            "\n",
            "[2025-09-20 00:18:00 IST] @KafiranaBlogs → भागवंत मान स्लैम्स सेंटर की of 1,600 करोड़ की सहायता - \n",
            "\n",
            " https://t.co/Bmwdk4hUjj \n",
            "\n",
            "#news #india https://t.co/z2SinEoopW\n",
            "\n",
            "[2025-09-20 00:17:31 IST] @BaazzOnHunt → Punjabis are fighting to curb illegal migration crisis\n",
            "@SukhpalKhaira \n",
            "demands controlled legal migration to prevent crime&amp; ensure safety..Bt Comrades wants to flood Punjab wth migrants from all states for personal gains.The clip of Marxist betrayal is wake up call for punjabis. https://t.co/FfdUaPPZgK\n",
            "\n",
            "Translating tweets to English...\n",
            "Classifying tweets for disaster type...\n",
            "\n",
            "=== Disaster Analysis from Tweets ===\n",
            "Final Disaster Type: flood\n",
            "Disaster Intensity (Confidence): 49.03%\n",
            "Detailed Scores per Hazard: {'flood': 49.032918214797974, 'tsunami': 0.0, 'storm': 6.501468420028686, 'earthquake': 0.0, 'cyclone': 0.0, 'landslide': 0.0, 'fire': 7.8387224674224845, 'oil spill': 0.0, 'shipwreck': 0.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('flood',\n",
              " 49.032918214797974,\n",
              " {'flood': 49.032918214797974,\n",
              "  'tsunami': 0.0,\n",
              "  'storm': 6.501468420028686,\n",
              "  'earthquake': 0.0,\n",
              "  'cyclone': 0.0,\n",
              "  'landslide': 0.0,\n",
              "  'fire': 7.8387224674224845,\n",
              "  'oil spill': 0.0,\n",
              "  'shipwreck': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final code working fine with streamlit web app UI (functioning)"
      ],
      "metadata": {
        "id": "Mq5sM-TDcre-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from PIL import Image, ExifTags\n",
        "from PIL.ExifTags import TAGS, GPSTAGS\n",
        "from google.cloud import translate_v2 as translate\n",
        "import tweepy\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# ---------------- SETUP ----------------\n",
        "st.set_page_config(page_title=\"Disaster Detection App\", layout=\"wide\")\n",
        "\n",
        "# Google Translate setup\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"sihproject-471916-514170694ad5.json\"\n",
        "translate_client = translate.Client()\n",
        "\n",
        "# Tweepy setup\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAIyi4AEAAAAAvT6sMfLiUl%2FPzDe%2BiCV4NkvNzbQ%3DfoX0qsgli8JaAQLp8dcYFi6yliDVqZf2TRURigrKxhSvgnXsPB\"\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# Hazard keywords\n",
        "hazard_keywords = [\"flood\", \"tsunami\", \"storm\", \"earthquake\", \"cyclone\", \"landslide\", \"fire\", \"oil spill\", \"shipwreck\"]\n",
        "\n",
        "# Zero-shot classifier setup\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# ---------------- EXIF + GPS Functions ----------------\n",
        "def get_exif_data(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    exif_data = {}\n",
        "    info = image._getexif()\n",
        "    if not info:\n",
        "        return None\n",
        "    for tag, value in info.items():\n",
        "        decoded = TAGS.get(tag, tag)\n",
        "        if decoded == \"GPSInfo\":\n",
        "            gps_data = {}\n",
        "            for t in value:\n",
        "                gps_decoded = GPSTAGS.get(t, t)\n",
        "                gps_data[gps_decoded] = value[t]\n",
        "            exif_data[decoded] = gps_data\n",
        "        else:\n",
        "            exif_data[decoded] = value\n",
        "    return exif_data\n",
        "\n",
        "def to_float(rational):\n",
        "    try:\n",
        "        return float(rational)\n",
        "    except TypeError:\n",
        "        return rational.numerator / rational.denominator\n",
        "\n",
        "def convert_to_degrees(value):\n",
        "    d, m, s = value\n",
        "    return to_float(d) + to_float(m)/60 + to_float(s)/3600\n",
        "\n",
        "def get_lat_lon(exif_data):\n",
        "    if not exif_data or \"GPSInfo\" not in exif_data:\n",
        "        return None, None\n",
        "    gps_info = exif_data[\"GPSInfo\"]\n",
        "    lat = convert_to_degrees(gps_info[\"GPSLatitude\"])\n",
        "    if gps_info[\"GPSLatitudeRef\"] != \"N\":\n",
        "        lat = -lat\n",
        "    lon = convert_to_degrees(gps_info[\"GPSLongitude\"])\n",
        "    if gps_info[\"GPSLongitudeRef\"] != \"E\":\n",
        "        lon = -lon\n",
        "    return lat, lon\n",
        "\n",
        "def reverse_geocode(lat, lon):\n",
        "    url = f\"https://nominatim.openstreetmap.org/reverse?lat={lat}&lon={lon}&format=json\"\n",
        "    response = requests.get(url, headers={\"User-Agent\": \"my-app\"}).json()\n",
        "    address = response.get(\"address\", {})\n",
        "    city = address.get(\"city\") or address.get(\"town\") or address.get(\"village\") or None\n",
        "    full_address = response.get(\"display_name\", \"Unknown\")\n",
        "    return city, full_address\n",
        "\n",
        "# ---------------- Hazard & Tweets ----------------\n",
        "def classify_hazard(description, target_language=\"en\"):\n",
        "    result = translate_client.translate(description, target_language=target_language)\n",
        "    translated_text = result[\"translatedText\"].lower()\n",
        "    scores = {}\n",
        "    for hazard in hazard_keywords:\n",
        "        scores[hazard] = 1.0 if hazard in translated_text else 0.0\n",
        "    best_hazard = max(scores, key=scores.get)\n",
        "    return best_hazard, scores\n",
        "\n",
        "def detect_city_from_text(text):\n",
        "    result = translate_client.translate(text, target_language=\"en\")\n",
        "    translated_text = result[\"translatedText\"].lower()\n",
        "    words = translated_text.split()\n",
        "    return words[0] if words else \"Unknown\"\n",
        "\n",
        "def fetch_tweets(city, hazard, max_results=10):\n",
        "    query = f'(\"{city}\" OR #{city}) (\"{hazard}\" OR #{hazard}) -is:retweet'\n",
        "\n",
        "    now = datetime.utcnow()\n",
        "    start_time = (now - timedelta(hours=1)).isoformat(\"T\") + \"Z\"\n",
        "    end_time   = (now - timedelta(seconds=15)).isoformat(\"T\") + \"Z\"\n",
        "\n",
        "    local_tz = pytz.timezone(\"Asia/Kolkata\")\n",
        "\n",
        "    tweets = client.search_recent_tweets(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        tweet_fields=[\"created_at\", \"text\", \"geo\"],\n",
        "        expansions=[\"author_id\"],\n",
        "        user_fields=[\"username\"],\n",
        "        start_time=start_time,\n",
        "        end_time=end_time\n",
        "    )\n",
        "\n",
        "    user_map = {}\n",
        "    if tweets.includes and \"users\" in tweets.includes:\n",
        "        for u in tweets.includes[\"users\"]:\n",
        "            user_map[u[\"id\"]] = u[\"username\"]\n",
        "\n",
        "    tweets_data = []\n",
        "    if tweets.data:\n",
        "        for t in tweets.data:\n",
        "            local_time = t.created_at.astimezone(local_tz)\n",
        "            formatted_time = local_time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "            username = user_map.get(t.author_id, \"Unknown\")\n",
        "            tweets_data.append({\"time\": formatted_time, \"user\": username, \"text\": t.text})\n",
        "    return tweets_data\n",
        "\n",
        "def translate_tweets_to_english(tweets_list):\n",
        "    translated = []\n",
        "    for tweet in tweets_list:\n",
        "        result = translate_client.translate(tweet[\"text\"], target_language=\"en\")\n",
        "        translated.append(result[\"translatedText\"])\n",
        "    return translated\n",
        "\n",
        "def classify_tweets_disaster(translated_tweets):\n",
        "    aggregate_scores = {hazard: 0.0 for hazard in hazard_keywords}\n",
        "    for tweet in translated_tweets:\n",
        "        res = classifier(tweet, hazard_keywords, multi_label=False)\n",
        "        top_label = res['labels'][0]\n",
        "        top_score = res['scores'][0]\n",
        "        aggregate_scores[top_label] += top_score\n",
        "    num_tweets = len(translated_tweets)\n",
        "    if num_tweets == 0:\n",
        "        return None, None, None\n",
        "    for hazard in aggregate_scores:\n",
        "        aggregate_scores[hazard] = (aggregate_scores[hazard] / num_tweets) * 100\n",
        "    final_disaster = max(aggregate_scores, key=aggregate_scores.get)\n",
        "    intensity = aggregate_scores[final_disaster]\n",
        "    return final_disaster, intensity, aggregate_scores\n",
        "\n",
        "# ---------------- Streamlit UI ----------------\n",
        "st.title(\"🌍 DisasterPulse\")\n",
        "\n",
        "mode = st.radio(\"Choose Input Mode\", [\"Image + Description\", \"Direct Text\"])\n",
        "\n",
        "tweets_data = []\n",
        "final_disaster, intensity, scores, city, full_address = None, None, None, None, None\n",
        "\n",
        "if mode == \"Direct Text\":\n",
        "    direct_text = st.text_area(\"Enter disaster-related text (any language):\")\n",
        "    if st.button(\"Analyze\"):\n",
        "        hazard, _ = classify_hazard(direct_text)\n",
        "        city = detect_city_from_text(direct_text)\n",
        "        st.write(f\"**Detected City:** {city}\")\n",
        "        st.write(f\"**Predicted Hazard:** {hazard}\")\n",
        "        tweets_data = fetch_tweets(city, hazard)\n",
        "        translated = translate_tweets_to_english(tweets_data)\n",
        "        final_disaster, intensity, scores = classify_tweets_disaster(translated)\n",
        "\n",
        "elif mode == \"Image + Description\":\n",
        "    uploaded_img = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "    description = st.text_area(\"Enter disaster description (any language):\")\n",
        "    if uploaded_img and description and st.button(\"Analyze\"):\n",
        "        img_path = \"temp_img.jpg\"\n",
        "        with open(img_path, \"wb\") as f:\n",
        "            f.write(uploaded_img.read())\n",
        "\n",
        "        exif_data = get_exif_data(img_path)\n",
        "        lat, lon = get_lat_lon(exif_data)\n",
        "        if lat and lon:\n",
        "            city, full_address = reverse_geocode(lat, lon)\n",
        "            st.write(f\"**Detected City:** {city}\")\n",
        "            st.write(f\"**Full Address:** {full_address}\")\n",
        "        else:\n",
        "            city = detect_city_from_text(description)\n",
        "            st.write(f\"**Detected City from text:** {city}\")\n",
        "\n",
        "        hazard, _ = classify_hazard(description)\n",
        "        st.write(f\"**Predicted Hazard:** {hazard}\")\n",
        "\n",
        "        tweets_data = fetch_tweets(city, hazard)\n",
        "        translated = translate_tweets_to_english(tweets_data)\n",
        "        final_disaster, intensity, scores = classify_tweets_disaster(translated)\n",
        "\n",
        "# ---------------- Show Results ----------------\n",
        "if final_disaster:\n",
        "    st.subheader(\"📊 Disaster Analysis Result\")\n",
        "    st.write(f\"**Final Disaster Type:** {final_disaster}\")\n",
        "    st.write(f\"**Confidence (Intensity):** {intensity:.2f}%\")\n",
        "    st.write(\"### Detailed Hazard Scores:\")\n",
        "    st.json(scores)\n",
        "\n",
        "    if tweets_data:\n",
        "        st.subheader(\"📝 Fetched Tweets\")\n",
        "        df = pd.DataFrame(tweets_data)\n",
        "        st.dataframe(df)\n",
        "\n",
        "        # Download option\n",
        "        csv = df.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\"Download Tweets as CSV\", data=csv, file_name=\"tweets.csv\", mime=\"text/csv\")\n",
        "    else:\n",
        "        st.warning(\"No tweets found for this query.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABiMvOiIZqvd",
        "outputId": "51dca30a-039e-4811-99fb-42d466991475"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 30BSvhKS0UakM7ZXSv2i1dIo0wf_2F2vBDU86aSMsAJCachbM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV_s9JWdZ-BR",
        "outputId": "9b598fec-e357-4753-b3be-48e1df7682a1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.port 8501 --server.headless true\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "CsdJqO8UaAdM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill previous tunnels (avoid conflicts)\n",
        "ngrok.kill()\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🚀 Your Streamlit app is live here:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V81C9zlpaKXK",
        "outputId": "fbefe39d-0f14-47ff-e009-7e92ed5db5e6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Your Streamlit app is live here: NgrokTunnel: \"https://adff13f19039.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}